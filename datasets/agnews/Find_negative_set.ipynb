{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I/ CONSTRUCTION OF THE SETS - SECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category Vocab Construction based on provided Keyword AND Construction of the positive set\n",
    "\n",
    "##### Need to provide the keywords in the file label_name.txt\n",
    "##### All the hyperparameters may be modified in the script agnews.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating the folder\n",
    "import os\n",
    "keyword = 'all_names'\n",
    "path = os.getcwd()+'/'+keyword+'/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/bastien/Documents/ProjetNLP/LOTClass/datasets/agnews/all_names/'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ export CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "+ CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "+ export CUDA_VISIBLE_DEVICES=0,1\n",
      "+ CUDA_VISIBLE_DEVICES=0,1\n",
      "+ DATASET=agnews\n",
      "+ LABEL_NAME_FILE=label_names.txt\n",
      "+ TRAIN_CORPUS=train.txt\n",
      "+ TEST_CORPUS=test.txt\n",
      "+ TEST_LABEL=test_labels.txt\n",
      "+ TRAIN_LABEL=train_labels.txt\n",
      "+ MAX_LEN=200\n",
      "+ TRAIN_BATCH=32\n",
      "+ ACCUM_STEP=4\n",
      "+ EVAL_BATCH=128\n",
      "+ GPUS=1\n",
      "+ MCP_EPOCH=3\n",
      "+ SELF_TRAIN_EPOCH=1\n",
      "+ python src/train.py --dataset_dir datasets/agnews/ --label_names_file label_names.txt --train_file train.txt --train_label_file train_labels.txt --test_file test.txt --test_label_file test_labels.txt --max_len 200 --train_batch_size 32 --accum_steps 4 --eval_batch_size 128 --gpus 1 --mcp_epochs 3 --self_train_epochs 1\n",
      "2020-11-27 13:21:52.260232: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "Namespace(accum_steps=4, category_vocab_size=100, dataset_dir='datasets/agnews/', dist_port=12345, early_stop=False, eval_batch_size=128, final_model='final_model.pt', gpus=1, label_names_file='label_names.txt', match_threshold=20, max_len=200, mcp_epochs=3, out_file='out.txt', self_train_epochs=1.0, test_file='test.txt', test_label_file='test_labels.txt', top_pred_num=50, train_batch_size=32, train_file='train.txt', train_label_file='train_labels.txt', update_interval=50)\n",
      "Effective training batch size: 128\n",
      "Label names used for each class are: {0: ['politics'], 1: ['sports'], 2: ['business'], 3: ['technology']}\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing LOTClassModel: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing LOTClassModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LOTClassModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LOTClassModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias', 'dense.weight', 'dense.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Reading texts from datasets/agnews/train.txt\n",
      "Converting texts into tensors.\n",
      "2020-11-27 13:21:58.102302: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2020-11-27 13:21:58.671813: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2020-11-27 13:21:59.267350: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2020-11-27 13:21:59.970318: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2020-11-27 13:22:00.644390: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2020-11-27 13:22:01.303972: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2020-11-27 13:22:02.088311: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2020-11-27 13:22:02.990796: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2020-11-27 13:22:03.836244: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2020-11-27 13:22:04.527368: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "Saving encoded texts into datasets/agnews/train.pt\n",
      "Reading labels from datasets/agnews/train_labels.txt\n",
      "Reading texts from datasets/agnews/train.txt\n",
      "Locating label names in the corpus.\n",
      "Saving texts with label names into datasets/agnews/label_name_data.pt\n",
      "Reading texts from datasets/agnews/test.txt\n",
      "Converting texts into tensors.\n",
      "Saving encoded texts into datasets/agnews/test.pt\n",
      "Reading labels from datasets/agnews/test_labels.txt\n",
      "Contructing category vocabulary.\n",
      "2020-11-27 13:22:42.754506: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "100%|███████████████████████████████████████████| 62/62 [02:20<00:00,  2.27s/it]\n",
      "Class 0 category vocabulary: ['politics', 'political', 'politicians', 'government', 'elections', 'politician', 'democracy', 'democratic', 'governing', 'party', 'state', 'leadership', 'election', 'politically', 'affairs', 'issues', 'governments', 'voters', 'debate', 'cabinet', 'congress', 'democrat', 'administration', 'president', 'religion', 'republican', 'history', 'crisis', 'war', 'legislature', 'candidates', 'governance', 'pr', 'opposition', 'problems', 'relations', 'finance', 'justice', 'struggle', 'rhetoric', 'right', 'convention', 'votes', 'fighting', 'violence', 'senate', 'matters', 'fight', 'us', 'parliament', 'republicans', 'trouble', 'one', 'conflict', 'soil', 'voting', 'law', 'parliamentary', 'representation', 'house', 'reality', 'wars', 'campaign', 'contest', 'candidate', 'campaigns', 'legislative', 'transition', 'question', 'choice']\n",
      "\n",
      "Class 1 category vocabulary: ['sports', 'games', 'sporting', 'athletics', 'game', 'national', 'news', 'athletic', 'espn', 'soccer', 'stadium', 'basketball', 'arts', 'racing', 'baseball', 'tv', 'hockey', 'pro', 'press', 'team', 'red', 'home', 'bay', 'kings', 'legends', 'city', 'winning', 'miracle', 'olympic', 'go', 'giants', 'champions', 'ball', 'players', 'boxing', 'prime', 'teams', 'athletes', 'tennis', 'club', 'blue', 'coaches', 'gold', 'west', 'toronto', 'classic', 'pittsburgh', 'super', 'nfl', 'magic', 'key', 'times', 'field', 'warriors', 'rogers', 'stars', 'gym', 'championship', 'losses', 'college', 'mlb', 'veterans', 'rugby', 'hits', 'sun', 'bc', 'events', 'south', 'nba']\n",
      "\n",
      "Class 2 category vocabulary: ['business', 'businesses', 'trade', 'commercial', 'enterprise', 'shop', 'money', 'market', 'commerce', 'corporate', 'global', 'future', 'sales', 'general', 'group', 'retail', 'companies', 'management', 'operations', 'operation', 'corporation', 'store', 'division', 'firm', 'venture', 'brand', 'contract', 'revenue', 'economic', 'branch', 'subsidiary', 'personal', 'cash', 'short', 'line', 'bank', 'customer', 'concern', 'growth', 'chain', 'strategic', 'family', 'work', 'products', 'big', 'scientific', 'virtual', 'engineering', 'sector', 'trading', 'portfolio', 'ceo', 'segment', 'investment', 'working', 'executive', 'private', 'services', 'public', 'job', 'marketing']\n",
      "\n",
      "Class 3 category vocabulary: ['technology', 'technologies', 'tech', 'software', 'technological', 'device', 'equipment', 'hardware', 'infrastructure', 'devices', 'system', 'knowledge', 'technique', 'digital', 'technical', 'concept', 'systems', 'gear', 'techniques', 'material', 'functionality', 'process', 'facility', 'feature', 'capability', 'content', 'method', 'security', 'ability', 'network', 'internet', 'computing', 'chip', 'smart', 'modern', 'communication', 'language', 'mechanism', 'computer', 'design', 'cyber', 'standard', 'tool', 'development', 'format', 'protocol', 'wireless', 'phone', 'information', 'program', 'ce', 'plant', 'large', 'data', 'project', 'application', 'theory', 'science', 'performance', 'common', 'os', 'ict', 'speed', 'sensor', 'capabilities', 'electronic', 'society', 'silicon', 'invention', 'memory']\n",
      "\n",
      "Preparing self supervision for masked category prediction.\n",
      "2020-11-27 13:25:06.954406: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "train label present in the dataset\n",
      "100%|█████████████████████████████████████████| 938/938 [36:36<00:00,  2.34s/it]\n",
      "Number of documents with category indicative terms found for each category is: {0: 211, 1: 1365, 2: 1426, 3: 2049}\n",
      "There are totally 5051 documents with category indicative terms.\n"
     ]
    }
   ],
   "source": [
    "!cd ../.. && bash -x agnews.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p all_names\n",
    "!mv *.pt all_names/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPORTS AND GLOBAL PARAMETER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import gc\n",
    "from joblib import Parallel, delayed\n",
    "import random\n",
    "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
    "from torch.utils.data import Subset\n",
    "import pickle as p\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from transformers import BertPreTrainedModel, BertModel\n",
    "from transformers.modeling_bert import BertOnlyMLMHead\n",
    "from torch import nn\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import pickle as p\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams \n",
    "from itertools import chain\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "positive_label = [0]\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistics of the Positive Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DATA FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp_data = torch.load(path+'mcp_train.pt')\n",
    "assumed_label = []\n",
    "for x in mcp_data['labels']:\n",
    "    if (x!=-1).sum().item() < 2:\n",
    "        assumed_label.append(((x!=-1)*x).sum().item())\n",
    "    elif len(np.unique(x[x!=-1].numpy())) == 1:       \n",
    "        assumed_label.append(np.unique(x[x!=-1].numpy())[0])\n",
    "    else:\n",
    "        assumed_label.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground = mcp_data['ground_truth'].numpy()\n",
    "assumed_label = np.array(assumed_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([ground, assumed_label]).T\n",
    "df.columns = ['ground','assumed']\n",
    "df['valid'] = df['ground']==df['assumed']\n",
    "precision = df.groupby('assumed')['valid'].sum()/df.groupby('assumed')['assumed'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assumed\n",
       "0    0.563981\n",
       "1    0.942857\n",
       "2    0.633240\n",
       "3    0.782821\n",
       "dtype: float64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp_data = torch.load(path+'mcp_train.pt')\n",
    "label = torch.LongTensor([1]).repeat(len(mcp_data['labels']))\n",
    "import pandas as pd\n",
    "number_of_correct_positives = pd.DataFrame(mcp_data['ground_truth'].numpy()).value_counts().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRECISION of the positive set :  0.7555228276877761\n",
      "NUMBER OF ELEMENTS in the positive set 4074\n"
     ]
    }
   ],
   "source": [
    "# real positive over positive \n",
    "print(\"PRECISION of the positive set : \", number_of_correct_positives[0]/len(mcp_data['ground_truth']))\n",
    "pos_set_accuracy = number_of_correct_positives[0]/len(mcp_data['ground_truth'])\n",
    "print(\"NUMBER OF ELEMENTS in the positive set\", len(mcp_data['ground_truth']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEFINITION OF THE MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LOTClassModel(BertPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.cls = BertOnlyMLMHead(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.init_weights()\n",
    "        # MLM head is not trained\n",
    "        for param in self.cls.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, input_ids, pred_mode, attention_mask=None, token_type_ids=None, \n",
    "                position_ids=None, head_mask=None, inputs_embeds=None):\n",
    "        bert_outputs = self.bert(input_ids,\n",
    "                                 attention_mask=attention_mask,\n",
    "                                 token_type_ids=token_type_ids,\n",
    "                                 position_ids=position_ids,\n",
    "                                 head_mask=head_mask,\n",
    "                                 inputs_embeds=inputs_embeds)\n",
    "        last_hidden_states = bert_outputs[0]\n",
    "        if pred_mode == \"classification\":\n",
    "            trans_states = self.dense(last_hidden_states)\n",
    "            trans_states = self.activation(trans_states)\n",
    "            trans_states = self.dropout(trans_states)\n",
    "            logits = self.classifier(trans_states)\n",
    "        elif pred_mode == \"mlm\":\n",
    "            logits = self.cls(last_hidden_states)\n",
    "        else:\n",
    "            sys.exit(\"Wrong pred_mode!\")\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEFINITION OF THE TOKENIZER FOR THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "vocab = tokenizer.get_vocab()\n",
    "inv_vocab = {k:v for v, k in vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPORTING THE DATA AND PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vocab = torch.load(path+\"category_vocab.pt\")\n",
    "label_data = torch.load(path+\"label_name_data.pt\")\n",
    "train_data = torch.load(path+'train.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEXT AND LABELS LOADING AND FORMATING\n",
    "corpus = open('train.txt', encoding=\"utf-8\")\n",
    "true_labels = open('train_labels.txt', encoding=\"utf-8\")\n",
    "docs_labels = [doc.strip() for doc in true_labels.readlines()]\n",
    "dict_label = {0:[], 1:[], 2:[],3:[]}\n",
    "list_label = [int(label) for label in docs_labels]\n",
    "for i, label in enumerate(docs_labels):\n",
    "    dict_label[int(label)].append(i)\n",
    "docs = [doc.strip() for doc in corpus.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ALL KEYWORDS' RELATED WORD ARE STACKED TOGETHER as Tokens (integers)\n",
    "category_vocab = []\n",
    "for k in data_vocab.keys():\n",
    "    category_vocab += list(data_vocab[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### CREATION OF THE LIST as Strings\n",
    "list_pos_keyword = []\n",
    "for w in category_vocab:\n",
    "    list_pos_keyword.append(inv_vocab[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEFINITION OF UTILITARY FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, number = 1024, test_batch_size = 32,docs = docs, all = False, true_label = positive_label):\n",
    "    model.eval()\n",
    "    true_negative = 0\n",
    "    true_positive = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "    correct_pred = 0\n",
    "    negative = 0\n",
    "    divider = number\n",
    "    if all:\n",
    "        test_list = list(range(len(docs)))\n",
    "        divider = len(docs)\n",
    "    else:\n",
    "        test_list = random.sample(list(range(len(docs))), k = number)\n",
    "    inputs = torch.stack([encode(docs[i])[0].squeeze() for i in test_list])\n",
    "    attention_mask = torch.stack([encode(docs[i])[1].squeeze() for i in test_list])\n",
    "    true_labels = torch.stack([torch.tensor(int(list_label[i] in true_label)) for i in test_list])\n",
    "    test_dataset = TensorDataset(inputs, attention_mask, true_labels)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size = test_batch_size)\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            inputs_test, attention_test, labels_test = batch\n",
    "            logits = model(inputs_test.to(device),attention_mask=attention_test.to(device), pred_mode='classification')\n",
    "            logits_cls = logits[:,0]\n",
    "            prediction = torch.argmax(logits_cls, -1)\n",
    "            \n",
    "            true_positive += (prediction.cpu()*labels_test).sum().item()\n",
    "            true_negative += ((1-prediction.cpu())*(1-labels_test)).sum().item()\n",
    "            false_positive += ((prediction.cpu())*(1-labels_test)).sum().item()\n",
    "            false_negative += ((1-prediction.cpu())*(labels_test)).sum().item()\n",
    "            correct_pred += (labels_test == prediction.cpu()).sum().item()\n",
    "            assert (correct_pred == (true_positive + true_negative))\n",
    "        assert(true_positive+true_negative+false_positive+false_negative == divider)\n",
    "        accuracy = correct_pred / divider\n",
    "        \n",
    "    if (true_positive+false_positive) > 0:\n",
    "        precision = true_positive / (true_positive+false_positive)\n",
    "        print('Precision', precision)\n",
    "    else : \n",
    "        precision = None\n",
    "        print(\"Precision Undefined\")\n",
    "    if (true_positive+false_negative) > 0 :\n",
    "        recall = true_positive/(true_positive+false_negative)\n",
    "        print('Recall', recall)\n",
    "    else :\n",
    "        recall = None\n",
    "        print(\"Recall Undefined\")\n",
    "    if recall+precision > 0:\n",
    "        f1_score = 2*(recall*precision)/(recall+precision)\n",
    "        print(\"F1_score\", f1_score)\n",
    "    else:\n",
    "        print(\"F1_score Undefined\")\n",
    "    print(\"Accuracy \", accuracy)\n",
    "    model.train()\n",
    "    return accuracy, precision, recall, f1_score\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def encode(docs, tokenizer = tokenizer, max_length = 200):\n",
    "    encoded_dict = tokenizer.encode_plus(docs, add_special_tokens=True, max_length=max_length, padding='max_length',\n",
    "                                                    return_attention_mask=True, truncation=True, return_tensors='pt')\n",
    "    input_ids = encoded_dict['input_ids']\n",
    "    attention_masks = encoded_dict['attention_mask']\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "\n",
    "\n",
    "#### THESE ARE ONLY USED WHEN COMPUTING INTERSECTION ON GPUs\n",
    "def intersect_tensor(t1, t2, device = 'cuda', mask = None):    \n",
    "    indices = torch.zeros_like(t1, dtype = torch.uint8, device = device)\n",
    "    for elem in t2:\n",
    "        indices = indices | (t1 == elem) \n",
    "        indices = indices.to(bool)\n",
    "        \n",
    "    if mask is not None:\n",
    "        indices = indices * mask \n",
    "    intersection = t1[indices]  \n",
    "    return intersection, indices\n",
    "\n",
    "def count_similar_words(batch, category_vocab = category_vocab):\n",
    "    prediction= batch[0]\n",
    "    input_mask = batch[1]\n",
    "    masked_pred = prediction[:input_mask.sum().item(),:]\n",
    "    _ , words = torch.topk(masked_pred, 8, -1)\n",
    "    counter = 0\n",
    "    for word in words.squeeze():\n",
    "        counter += int(len(np.intersect1d(word.numpy(), category_vocab))>0)\n",
    "        intersect_time = time() - intersect_time_start\n",
    "        if counter > 0:\n",
    "            print('break')\n",
    "            return False\n",
    "            break\n",
    "    return True\n",
    "            \n",
    "def occurences(word, vocab = category_vocab):\n",
    "    return len(np.intersect1d(word.cpu().numpy(), vocab))\n",
    "\n",
    "def decode(ids, tokenizer=tokenizer):\n",
    "    strings = tokenizer.batch_decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    return strings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SELECTION OF TEXTS WITH NO KEYWORDS' RELATED WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120000it [01:08, 1761.08it/s]\n"
     ]
    }
   ],
   "source": [
    "negative_doc=[]\n",
    "negative_doc_label = []\n",
    "for k, doc in tqdm(enumerate(docs)):\n",
    "    tokenized_doc = tokenizer.tokenize(doc)\n",
    "    new_doc = []\n",
    "    wordpcs = []\n",
    "    label_idx = -1 * torch.ones(512, dtype=torch.long)\n",
    "    for idx, wordpc in enumerate(tokenized_doc):\n",
    "        wordpcs.append(wordpc[2:] if wordpc.startswith(\"##\") else wordpc)\n",
    "        if idx >= 512 - 1: # last index will be [SEP] token\n",
    "            break\n",
    "        if idx == len(doc) - 1 or not doc[idx+1].startswith(\"##\"):\n",
    "            word = ''.join(wordpcs)\n",
    "            if word in list_pos_keyword:\n",
    "                label_idx[idx] = 0\n",
    "                break\n",
    "            new_word = ''.join(wordpcs)\n",
    "            if new_word != tokenizer.unk_token:\n",
    "                idx += len(wordpcs)\n",
    "                new_doc.append(new_word)\n",
    "            wordpcs = []\n",
    "    if (label_idx>=0).any():\n",
    "        continue\n",
    "    else:\n",
    "        negative_doc_label.append(list_label[k])\n",
    "        negative_doc.append(doc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics of the Negative Set before the use of any language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative pre-set 64365\n",
      "Precision pre-set,  0.8848753204381263\n"
     ]
    }
   ],
   "source": [
    "print(\"Negative pre-set\", len(negative_doc))\n",
    "print(\"Precision pre-set, \", len([k for k in negative_doc_label if k not in positive_label])/len(negative_doc_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FORMATING THE NEGATIVE SET - ENCODING AND FORMATING INTO TENSORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64365/64365 [00:35<00:00, 1824.58it/s]\n"
     ]
    }
   ],
   "source": [
    "inputs_list = []\n",
    "masks_list = []\n",
    "for doc in tqdm(negative_doc):\n",
    "    input_ids, input_mask = encode(doc)\n",
    "    inputs_list.append(input_ids)\n",
    "    masks_list.append(input_mask)\n",
    "input_tensor = torch.stack(inputs_list).squeeze()\n",
    "mask_tensor = torch.stack(masks_list).squeeze()\n",
    "label_tensor = torch.stack([torch.tensor(i).unsqueeze(0) for i in negative_doc_label])\n",
    "dataset = torch.utils.data.TensorDataset(input_tensor,mask_tensor, label_tensor)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, shuffle = False, batch_size = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CREATION OF MODEL AND HYPERPARAMETERS FOR THE FILTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing LOTClassModel: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing LOTClassModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LOTClassModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LOTClassModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias', 'dense.weight', 'dense.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = LOTClassModel.from_pretrained('bert-base-uncased',\n",
    "                                           output_attentions=False,\n",
    "                                           output_hidden_states=False,\n",
    "                                           num_labels=2).to('cuda')\n",
    "\n",
    "verified_negative = []\n",
    "correct_label = 0\n",
    "verbose = True\n",
    "topk = 15\n",
    "vocab = torch.tensor(category_vocab).to(device)\n",
    "min_similar_words = 0\n",
    "max_category_word = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COMPUTING THE NEGATIVE SET BASED ON THE PRETRAINED LANGUAGE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102it [00:16,  6.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9636363636363636\n",
      "number of elements retrieved 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "202it [00:33,  5.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.975609756097561\n",
      "number of elements retrieved 123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "302it [00:50,  5.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9723756906077348\n",
      "number of elements retrieved 181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "402it [01:07,  5.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9730769230769231\n",
      "number of elements retrieved 260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "502it [01:24,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9771428571428571\n",
      "number of elements retrieved 350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "602it [01:41,  5.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9805352798053528\n",
      "number of elements retrieved 411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "702it [01:58,  5.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9786780383795309\n",
      "number of elements retrieved 469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "802it [02:15,  5.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9751434034416826\n",
      "number of elements retrieved 523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "902it [02:32,  5.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9783333333333334\n",
      "number of elements retrieved 600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1002it [02:49,  5.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.977810650887574\n",
      "number of elements retrieved 676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1102it [03:06,  5.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9779917469050894\n",
      "number of elements retrieved 727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1202it [03:24,  5.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.978343949044586\n",
      "number of elements retrieved 785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1302it [03:41,  5.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9791666666666666\n",
      "number of elements retrieved 864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1402it [03:58,  5.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.977997799779978\n",
      "number of elements retrieved 909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1502it [04:15,  5.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9791666666666666\n",
      "number of elements retrieved 960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1602it [04:32,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9782823297137216\n",
      "number of elements retrieved 1013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1702it [04:49,  5.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9775491113189897\n",
      "number of elements retrieved 1069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1802it [05:06,  5.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9768270944741533\n",
      "number of elements retrieved 1122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1902it [05:23,  5.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9769033361847733\n",
      "number of elements retrieved 1169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2002it [05:40,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.977850697292863\n",
      "number of elements retrieved 1219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2102it [05:57,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.978988326848249\n",
      "number of elements retrieved 1285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2202it [06:14,  5.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9802052785923754\n",
      "number of elements retrieved 1364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2302it [06:31,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9811977715877437\n",
      "number of elements retrieved 1436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2402it [06:48,  5.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9819277108433735\n",
      "number of elements retrieved 1494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2502it [07:05,  5.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9825581395348837\n",
      "number of elements retrieved 1548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2602it [07:22,  5.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9825436408977556\n",
      "number of elements retrieved 1604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2702it [07:39,  5.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9826034793041392\n",
      "number of elements retrieved 1667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2802it [07:56,  5.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9826086956521739\n",
      "number of elements retrieved 1725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2902it [08:13,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.983249581239531\n",
      "number of elements retrieved 1791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3002it [08:30,  5.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9826558265582656\n",
      "number of elements retrieved 1845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3102it [08:47,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9825489159175039\n",
      "number of elements retrieved 1891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3202it [09:04,  5.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9827147941026945\n",
      "number of elements retrieved 1967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3302it [09:21,  5.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9821073558648111\n",
      "number of elements retrieved 2012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3402it [09:38,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9800292255236239\n",
      "number of elements retrieved 2053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3502it [09:55,  5.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9805502846299811\n",
      "number of elements retrieved 2108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3602it [10:12,  5.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9782909930715935\n",
      "number of elements retrieved 2165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3702it [10:29,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9788383610986042\n",
      "number of elements retrieved 2221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3802it [10:46,  5.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9795296167247387\n",
      "number of elements retrieved 2296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3902it [11:03,  5.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9795221843003413\n",
      "number of elements retrieved 2344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4001it [11:21,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9796257796257797\n",
      "number of elements retrieved 2405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4102it [11:38,  5.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9789217673287394\n",
      "number of elements retrieved 2467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4202it [11:55,  5.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9791666666666666\n",
      "number of elements retrieved 2544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4302it [12:12,  5.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9793656858998854\n",
      "number of elements retrieved 2617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4402it [12:29,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9786116322701689\n",
      "number of elements retrieved 2665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4502it [12:46,  5.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9786056805606788\n",
      "number of elements retrieved 2711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4602it [13:03,  5.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9772972972972973\n",
      "number of elements retrieved 2775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4702it [13:20,  5.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9777070063694268\n",
      "number of elements retrieved 2826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4802it [13:37,  5.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9777391304347826\n",
      "number of elements retrieved 2875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4902it [13:54,  5.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9771798365122616\n",
      "number of elements retrieved 2936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5002it [14:12,  5.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9766042780748663\n",
      "number of elements retrieved 2992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5102it [14:29,  5.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9757058437294813\n",
      "number of elements retrieved 3046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5202it [14:46,  5.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.975953831356204\n",
      "number of elements retrieved 3119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5302it [15:02,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9755255726388453\n",
      "number of elements retrieved 3187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5402it [15:20,  5.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9753542821934689\n",
      "number of elements retrieved 3246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5502it [15:37,  5.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9748561042108452\n",
      "number of elements retrieved 3301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5602it [15:54,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.975356294536817\n",
      "number of elements retrieved 3368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5702it [16:11,  5.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.974824355971897\n",
      "number of elements retrieved 3416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5802it [16:28,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9746689694876224\n",
      "number of elements retrieved 3474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5902it [16:45,  5.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9747946757292552\n",
      "number of elements retrieved 3531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6002it [17:02,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9751604800446553\n",
      "number of elements retrieved 3583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6102it [17:18,  5.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9755225522552256\n",
      "number of elements retrieved 3636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6202it [17:35,  5.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9754901960784313\n",
      "number of elements retrieved 3672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6302it [17:52,  5.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9752088385879817\n",
      "number of elements retrieved 3711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6402it [18:09,  5.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9750729249535932\n",
      "number of elements retrieved 3771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6502it [18:26,  5.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9753926701570681\n",
      "number of elements retrieved 3820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6602it [18:43,  5.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9755784061696658\n",
      "number of elements retrieved 3890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6702it [19:00,  5.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9753994420492011\n",
      "number of elements retrieved 3943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6802it [19:17,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.97475\n",
      "number of elements retrieved 4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6902it [19:34,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9750801875154207\n",
      "number of elements retrieved 4053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7002it [19:51,  5.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9754973313925279\n",
      "number of elements retrieved 4122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7102it [20:08,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9749759384023099\n",
      "number of elements retrieved 4156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7202it [20:25,  5.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9750178443968593\n",
      "number of elements retrieved 4203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7302it [20:42,  5.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.974900304949566\n",
      "number of elements retrieved 4263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7402it [20:59,  5.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9747392815758981\n",
      "number of elements retrieved 4315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7502it [21:16,  5.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9749021413769284\n",
      "number of elements retrieved 4343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7602it [21:33,  5.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9743473325766174\n",
      "number of elements retrieved 4405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7702it [21:49,  5.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9744852282900627\n",
      "number of elements retrieved 4468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7802it [22:06,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9745406243081691\n",
      "number of elements retrieved 4517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7902it [22:23,  5.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9745502413339184\n",
      "number of elements retrieved 4558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8002it [22:40,  5.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9747826086956521\n",
      "number of elements retrieved 4600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8046it [22:47,  5.88it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for k, batch in tqdm(enumerate(dataloader)):\n",
    "\n",
    "        input_ids, input_mask, label_id = batch\n",
    "        predictions = model(input_ids.to(device),\n",
    "                        pred_mode=\"mlm\",\n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=input_mask.to(device))\n",
    "        # Loop over the documents in the batch\n",
    "        for i, doc in enumerate(predictions.cpu()):\n",
    "            # Selecting only the position corresponding to a word not the PADDING\n",
    "            masked_pred = doc[:input_mask[i].sum().item(),:]\n",
    "            # Selecting the TOP 'k' words predicted at each position\n",
    "            _ , words = torch.topk(masked_pred, topk, -1)\n",
    "            counter = 0\n",
    "            # Loop over the words in each document\n",
    "            for word in words.squeeze():\n",
    "                counter += int(len(np.intersect1d(word.cpu().numpy(), category_vocab))>min_similar_words)\n",
    "                if counter > max_category_word:\n",
    "                    break\n",
    "            if counter <= max_category_word:             \n",
    "                verified_negative.append(k*4+i)\n",
    "                if label_id[i] not in positive_label:\n",
    "                    correct_label += 1             \n",
    "        if k%100 == 0 and verbose:\n",
    "            if len(verified_negative)>0:\n",
    "                print('accuracy :', correct_label/len(verified_negative))\n",
    "                print('number of elements retrieved', len(verified_negative))\n",
    "\n",
    "\n",
    "neg_set_accuracy = correct_label/len(verified_negative)    \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXPORT THE SET AND THE DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.dump(verified_negative, open(path+'verified_negative.p','wb'))\n",
    "p.dump(dataloader, open(path+'dataloader.p','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II/ TRAINING SECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOADING OF THE SETS AND FORMATING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative Set\n",
    "new_verified_negative = p.load(open(path+'verified_negative.p','rb'))\n",
    "new_dataloader = p.load(open(path+'dataloader.p','rb'))\n",
    "\n",
    "# Positive Set\n",
    "mcp_data = torch.load(path+'mcp_train.pt')\n",
    "label = torch.LongTensor([1]).repeat(len(mcp_data['labels']))\n",
    "\n",
    "# Formating\n",
    "negative_dataset = Subset(new_dataloader.dataset, new_verified_negative)\n",
    "positive_dataset = torch.utils.data.TensorDataset(mcp_data['input_ids'], mcp_data['attention_masks'], mcp_data['labels'])\n",
    "\n",
    "## TO DO : Words statistics on both sets#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('(', 41106),\n",
       " (')', 40787),\n",
       " ('-', 39111),\n",
       " ('#39;s', 31080),\n",
       " ('new', 21157),\n",
       " ('.', 18231),\n",
       " ('reuters', 17192),\n",
       " ('said', 16863),\n",
       " ('ap', 16080),\n",
       " ('us', 12047),\n",
       " ('first', 8934),\n",
       " ('two', 8867),\n",
       " ('--', 7958),\n",
       " ('oil', 6972),\n",
       " ('u.s.', 6924),\n",
       " ('world', 6843),\n",
       " ('one', 6746),\n",
       " ('last', 6237),\n",
       " ('company', 5900),\n",
       " ('york', 5543),\n",
       " ('inc.', 5480),\n",
       " ('president', 5458),\n",
       " ('microsoft', 5191),\n",
       " ('monday', 5038),\n",
       " ('#39;', 5022),\n",
       " ('united', 4979),\n",
       " ('wednesday', 4901),\n",
       " ('tuesday', 4834),\n",
       " ('could', 4826),\n",
       " ('would', 4767),\n",
       " ('three', 4765),\n",
       " ('thursday', 4754),\n",
       " ('million', 4710),\n",
       " ('says', 4508),\n",
       " ('afp', 4484),\n",
       " ('may', 4438),\n",
       " ('friday', 4290),\n",
       " ('government', 4260),\n",
       " ('iraq', 4221),\n",
       " ('security', 4212),\n",
       " ('game', 4152),\n",
       " ('prices', 4128),\n",
       " ('next', 4110),\n",
       " ('yesterday', 4110),\n",
       " ('group', 4043),\n",
       " ('corp.', 4015),\n",
       " ('people', 3893),\n",
       " ('quot;', 3891),\n",
       " ('time', 3756),\n",
       " ('software', 3753),\n",
       " ('back', 3730),\n",
       " ('percent', 3626),\n",
       " ('second', 3571),\n",
       " ('win', 3561),\n",
       " ('internet', 3486)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### STATISTICS ON CORPUS\n",
    "corpus = open('train.txt', encoding=\"utf-8\")\n",
    "stopwords_vocab = stopwords.words('english')\n",
    "lines = corpus.readlines()\n",
    "words = chain.from_iterable(line.lower().split() for line in lines)\n",
    "count = Counter(word for word in words if word not in stopwords_vocab)\n",
    "count.most_common(55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CONSTRUCTION OF THE DATASETS AND OF THE WEIGHTED SAMPLER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Construction of the weighted sampler based on sets' sizes and of the target vector #########\n",
    "\n",
    "target = np.hstack((np.zeros(int(len(negative_dataset)), dtype=np.int32),\n",
    "                    np.ones(int(len(positive_dataset)), dtype=np.int32)))\n",
    "\n",
    "class_sample_count = np.array([len(np.where(target == t)[0]) for t in np.unique(target)])\n",
    "weight = 1. / class_sample_count\n",
    "samples_weight = np.array([weight[t] for t in target])\n",
    "\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "samples_weigth = samples_weight.double()\n",
    "\n",
    "\n",
    "target = torch.from_numpy(target).long()\n",
    "sampler = WeightedRandomSampler(samples_weight, len(samples_weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEFINTION OF THE BATCH SIZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CONSTRUCTION OF THE DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.stack([negative_data[0][:200] for negative_data in negative_dataset] + \n",
    "            [positive_data[0][:200] for positive_data in positive_dataset])\n",
    "\n",
    "mask = torch.stack([negative_data[1][:200] for negative_data in negative_dataset] + \n",
    "            [positive_data[1][:200] for positive_data in positive_dataset])\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(data,mask, target)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, sampler=sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MODEL INSTANTIATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing LOTClassModel: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing LOTClassModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LOTClassModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LOTClassModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias', 'dense.weight', 'dense.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = LOTClassModel.from_pretrained('bert-base-uncased',\n",
    "                                           output_attentions=False,\n",
    "                                           output_hidden_states=False,\n",
    "                                           num_labels=2).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAINING HYPERPARAMETERS AND PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "accum_steps = 8\n",
    "epochs = 3\n",
    "learning_rate = 1e-5\n",
    "train_loss = nn.CrossEntropyLoss()\n",
    "total_steps = len(train_loader) * epochs / accum_steps\n",
    "number_of_mask = 1\n",
    "\n",
    "parameters = {'epochs':epochs, 'learning_rate':learning_rate, 'number_of_mask':number_of_mask,\n",
    "              'accum_steps':accum_steps, \n",
    "              'batch_size': batch_size, \n",
    "              'pos_set' : len(positive_dataset),\n",
    "              'pos_set_accuracy' : pos_set_accuracy,\n",
    "              'neg_set' : len(negative_dataset),\n",
    "              'neg_set_accuracy' :neg_set_accuracy}\n",
    "\n",
    "\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.1*total_steps, num_training_steps=total_steps)\n",
    "\n",
    "number_of_mask = 1 \n",
    "# Metrics\n",
    "losses_track = []\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "global_steps = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  0\n",
      "loss tensor(0.6848, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.27303754266211605\n",
      "Recall 0.6037735849056604\n",
      "F1_score 0.3760282021151587\n",
      "Accuracy  0.4814453125\n",
      "loss tensor(0.6928, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.3063063063063063\n",
      "Recall 0.604982206405694\n",
      "F1_score 0.40669856459330145\n",
      "Accuracy  0.515625\n",
      "loss tensor(0.6855, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.2932330827067669\n",
      "Recall 0.639344262295082\n",
      "F1_score 0.4020618556701031\n",
      "Accuracy  0.546875\n",
      "loss tensor(0.6808, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.3691148775894539\n",
      "Recall 0.7452471482889734\n",
      "F1_score 0.4937027707808565\n",
      "Accuracy  0.607421875\n",
      "loss tensor(0.6557, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.4222222222222222\n",
      "Recall 0.7723577235772358\n",
      "F1_score 0.5459770114942528\n",
      "Accuracy  0.69140625\n",
      "loss tensor(0.6307, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.5851393188854489\n",
      "Recall 0.7132075471698113\n",
      "F1_score 0.6428571428571428\n",
      "Accuracy  0.794921875\n",
      "loss tensor(0.5672, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.6570397111913358\n",
      "Recall 0.7338709677419355\n",
      "F1_score 0.6933333333333334\n",
      "Accuracy  0.8427734375\n",
      "loss tensor(0.4572, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.6101083032490975\n",
      "Recall 0.7477876106194691\n",
      "F1_score 0.6719681908548708\n",
      "Accuracy  0.8388671875\n",
      "loss tensor(0.4479, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.6201298701298701\n",
      "Recall 0.7958333333333333\n",
      "F1_score 0.6970802919708029\n",
      "Accuracy  0.837890625\n",
      "loss tensor(0.3357, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.6412698412698413\n",
      "Recall 0.7952755905511811\n",
      "F1_score 0.710017574692443\n",
      "Accuracy  0.8388671875\n",
      "loss tensor(0.2736, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.6053412462908012\n",
      "Recall 0.7846153846153846\n",
      "F1_score 0.6834170854271358\n",
      "Accuracy  0.8154296875\n",
      "loss tensor(0.1646, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.6502890173410405\n",
      "Recall 0.8211678832116789\n",
      "F1_score 0.7258064516129032\n",
      "Accuracy  0.833984375\n",
      "loss tensor(0.3323, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.65\n",
      "Recall 0.708171206225681\n",
      "F1_score 0.6778398510242086\n",
      "Accuracy  0.8310546875\n",
      "loss tensor(0.3531, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.6643598615916955\n",
      "Recall 0.7111111111111111\n",
      "F1_score 0.6869409660107334\n",
      "Accuracy  0.8291015625\n",
      "loss tensor(0.2340, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.6909090909090909\n",
      "Recall 0.7335907335907336\n",
      "F1_score 0.7116104868913858\n",
      "Accuracy  0.849609375\n",
      "loss tensor(0.1148, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.7073170731707317\n",
      "Recall 0.725\n",
      "F1_score 0.7160493827160495\n",
      "Accuracy  0.865234375\n",
      "loss tensor(0.1120, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.7215189873417721\n",
      "Recall 0.6732283464566929\n",
      "F1_score 0.6965376782077393\n",
      "Accuracy  0.8544921875\n",
      "loss tensor(0.0596, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.7587548638132295\n",
      "Recall 0.7358490566037735\n",
      "F1_score 0.7471264367816092\n",
      "Accuracy  0.87109375\n",
      "loss tensor(0.0531, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.7377049180327869\n",
      "Recall 0.6896551724137931\n",
      "F1_score 0.712871287128713\n",
      "Accuracy  0.8583984375\n",
      "loss tensor(0.0334, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.7766990291262136\n",
      "Recall 0.6866952789699571\n",
      "F1_score 0.7289293849658314\n",
      "Accuracy  0.8837890625\n",
      "loss tensor(0.0253, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.7631578947368421\n",
      "Recall 0.6718146718146718\n",
      "F1_score 0.7145790554414785\n",
      "Accuracy  0.8642578125\n",
      "loss tensor(0.0296, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.7402597402597403\n",
      "Recall 0.6551724137931034\n",
      "F1_score 0.6951219512195121\n",
      "Accuracy  0.853515625\n",
      "loss tensor(0.2425, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.7366071428571429\n",
      "Recall 0.64453125\n",
      "F1_score 0.6875000000000001\n",
      "Accuracy  0.853515625\n",
      "Average training loss: 0.3470520079135895\n",
      "Epoch :  1\n",
      "loss tensor(0.1086, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.8200836820083682\n",
      "Recall 0.7075812274368231\n",
      "F1_score 0.7596899224806202\n",
      "Accuracy  0.87890625\n",
      "loss tensor(0.0255, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.8058252427184466\n",
      "Recall 0.6484375\n",
      "F1_score 0.7186147186147185\n",
      "Accuracy  0.873046875\n",
      "loss tensor(0.0236, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.8064516129032258\n",
      "Recall 0.5813953488372093\n",
      "F1_score 0.6756756756756758\n",
      "Accuracy  0.8359375\n",
      "loss tensor(0.0648, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.8248587570621468\n",
      "Recall 0.5407407407407407\n",
      "F1_score 0.6532438478747203\n",
      "Accuracy  0.8486328125\n",
      "loss tensor(0.0113, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.8407643312101911\n",
      "Recall 0.5136186770428015\n",
      "F1_score 0.6376811594202898\n",
      "Accuracy  0.853515625\n",
      "loss tensor(0.0253, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.84375\n",
      "Recall 0.5133079847908745\n",
      "F1_score 0.6382978723404255\n",
      "Accuracy  0.8505859375\n",
      "loss tensor(0.0193, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.8741721854304636\n",
      "Recall 0.5387755102040817\n",
      "F1_score 0.6666666666666667\n",
      "Accuracy  0.87109375\n",
      "loss tensor(0.0154, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.8070175438596491\n",
      "Recall 0.5655737704918032\n",
      "F1_score 0.6650602409638554\n",
      "Accuracy  0.8642578125\n",
      "loss tensor(0.0100, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.8043478260869565\n",
      "Recall 0.5606060606060606\n",
      "F1_score 0.6607142857142857\n",
      "Accuracy  0.8515625\n",
      "loss tensor(0.0085, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.7524752475247525\n",
      "Recall 0.6007905138339921\n",
      "F1_score 0.6681318681318681\n",
      "Accuracy  0.8525390625\n",
      "loss tensor(0.0310, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.7989130434782609\n",
      "Recall 0.5903614457831325\n",
      "F1_score 0.6789838337182448\n",
      "Accuracy  0.8642578125\n",
      "loss tensor(0.0184, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.8186528497409327\n",
      "Recall 0.587360594795539\n",
      "F1_score 0.6839826839826839\n",
      "Accuracy  0.857421875\n",
      "loss tensor(0.0076, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.7771084337349398\n",
      "Recall 0.5330578512396694\n",
      "F1_score 0.6323529411764707\n",
      "Accuracy  0.853515625\n",
      "loss tensor(0.0064, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.8064516129032258\n",
      "Recall 0.49019607843137253\n",
      "F1_score 0.6097560975609755\n",
      "Accuracy  0.84375\n",
      "loss tensor(0.0062, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.821917808219178\n",
      "Recall 0.48\n",
      "F1_score 0.6060606060606061\n",
      "Accuracy  0.84765625\n",
      "loss tensor(0.0092, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.8091603053435115\n",
      "Recall 0.4649122807017544\n",
      "F1_score 0.5905292479108635\n",
      "Accuracy  0.8564453125\n",
      "loss tensor(0.0068, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.7569444444444444\n",
      "Recall 0.44129554655870445\n",
      "F1_score 0.557544757033248\n",
      "Accuracy  0.8310546875\n",
      "loss tensor(0.0067, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.8333333333333334\n",
      "Recall 0.4927007299270073\n",
      "F1_score 0.6192660550458716\n",
      "Accuracy  0.837890625\n",
      "loss tensor(0.0064, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.7697368421052632\n",
      "Recall 0.4957627118644068\n",
      "F1_score 0.6030927835051547\n",
      "Accuracy  0.849609375\n",
      "loss tensor(0.0945, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.8034682080924855\n",
      "Recall 0.5091575091575091\n",
      "F1_score 0.6233183856502242\n",
      "Accuracy  0.8359375\n",
      "loss tensor(0.0055, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.7953216374269005\n",
      "Recall 0.5596707818930041\n",
      "F1_score 0.6570048309178743\n",
      "Accuracy  0.861328125\n",
      "loss tensor(0.2982, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.7941176470588235\n",
      "Recall 0.5465587044534413\n",
      "F1_score 0.6474820143884893\n",
      "Accuracy  0.8564453125\n",
      "loss tensor(0.0436, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.8224852071005917\n",
      "Recall 0.5325670498084292\n",
      "F1_score 0.6465116279069768\n",
      "Accuracy  0.8515625\n",
      "Average training loss: 0.03214210644364357\n",
      "Epoch :  2\n",
      "loss tensor(0.0099, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.7954545454545454\n",
      "Recall 0.5185185185185185\n",
      "F1_score 0.6278026905829596\n",
      "Accuracy  0.837890625\n",
      "loss tensor(0.0050, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.8198757763975155\n",
      "Recall 0.528\n",
      "F1_score 0.6423357664233577\n",
      "Accuracy  0.8564453125\n",
      "loss tensor(0.0045, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.8255813953488372\n",
      "Recall 0.5071428571428571\n",
      "F1_score 0.6283185840707964\n",
      "Accuracy  0.8359375\n",
      "loss tensor(0.0047, device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision 0.8145695364238411\n",
      "Recall 0.4939759036144578\n",
      "F1_score 0.615\n",
      "Accuracy  0.849609375\n",
      "loss tensor(0.0043, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.7705882352941177\n",
      "Recall 0.5219123505976095\n",
      "F1_score 0.6223277909738718\n",
      "Accuracy  0.8447265625\n",
      "loss tensor(0.0069, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.7719298245614035\n",
      "Recall 0.4925373134328358\n",
      "F1_score 0.6013667425968109\n",
      "Accuracy  0.8291015625\n",
      "loss tensor(0.0049, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.8292682926829268\n",
      "Recall 0.4689655172413793\n",
      "F1_score 0.5991189427312775\n",
      "Accuracy  0.822265625\n",
      "loss tensor(0.0172, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.7843137254901961\n",
      "Recall 0.47619047619047616\n",
      "F1_score 0.5925925925925926\n",
      "Accuracy  0.8388671875\n",
      "loss tensor(0.0037, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.8475609756097561\n",
      "Recall 0.5018050541516246\n",
      "F1_score 0.6303854875283447\n",
      "Accuracy  0.8408203125\n",
      "loss tensor(0.0087, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.7909604519774012\n",
      "Recall 0.5223880597014925\n",
      "F1_score 0.6292134831460674\n",
      "Accuracy  0.8388671875\n",
      "loss tensor(0.1562, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.7417218543046358\n",
      "Recall 0.4426877470355731\n",
      "F1_score 0.5544554455445545\n",
      "Accuracy  0.82421875\n",
      "loss tensor(0.0068, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.7482993197278912\n",
      "Recall 0.45454545454545453\n",
      "F1_score 0.5655526992287918\n",
      "Accuracy  0.8349609375\n",
      "loss tensor(0.0056, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.7848101265822784\n",
      "Recall 0.5145228215767634\n",
      "F1_score 0.6215538847117794\n",
      "Accuracy  0.8525390625\n",
      "loss tensor(0.0070, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.7834394904458599\n",
      "Recall 0.5\n",
      "F1_score 0.6104218362282878\n",
      "Accuracy  0.8466796875\n",
      "loss tensor(0.0045, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.8\n",
      "Recall 0.5057471264367817\n",
      "F1_score 0.619718309859155\n",
      "Accuracy  0.841796875\n",
      "loss tensor(0.0033, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.7965116279069767\n",
      "Recall 0.5269230769230769\n",
      "F1_score 0.6342592592592593\n",
      "Accuracy  0.845703125\n",
      "loss tensor(0.0051, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.8352272727272727\n",
      "Recall 0.5345454545454545\n",
      "F1_score 0.6518847006651886\n",
      "Accuracy  0.8466796875\n",
      "loss tensor(0.0039, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.7577639751552795\n",
      "Recall 0.48412698412698413\n",
      "F1_score 0.5907990314769975\n",
      "Accuracy  0.8349609375\n",
      "loss tensor(0.0038, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.7771084337349398\n",
      "Recall 0.5352697095435685\n",
      "F1_score 0.6339066339066338\n",
      "Accuracy  0.8544921875\n",
      "loss tensor(0.0043, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.7933333333333333\n",
      "Recall 0.49583333333333335\n",
      "F1_score 0.6102564102564103\n",
      "Accuracy  0.8515625\n",
      "loss tensor(0.0041, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.7616279069767442\n",
      "Recall 0.515748031496063\n",
      "F1_score 0.6150234741784038\n",
      "Accuracy  0.83984375\n",
      "loss tensor(0.0038, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.8404255319148937\n",
      "Recall 0.5766423357664233\n",
      "F1_score 0.683982683982684\n",
      "Accuracy  0.857421875\n",
      "loss tensor(0.0053, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Precision 0.8106508875739645\n",
      "Recall 0.5229007633587787\n",
      "F1_score 0.6357308584686776\n",
      "Accuracy  0.8466796875\n",
      "Average training loss: 0.014452052302658558\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "try:\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        model.zero_grad()\n",
    "        print('Epoch : ', i)\n",
    "        for j, batch in enumerate(train_loader):\n",
    "            input_ids = batch[0].to(device)\n",
    "            input_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "\n",
    "\n",
    "            ### RANDOM MASKING\n",
    "            random_masking = random.choices(list(range(199)),k=number_of_mask*input_ids.size(0))\n",
    "            for i, mask_pos in enumerate(random_masking):\n",
    "                input_ids[i%input_ids.size(0),mask_pos+1] = tokenizer.get_vocab()[tokenizer.mask_token]\n",
    "            \n",
    "            ### PREDICTION\n",
    "            logits = model(input_ids, \n",
    "                           pred_mode=\"classification\",\n",
    "                           token_type_ids=None, \n",
    "                           attention_mask=input_mask)\n",
    "            ### LOSS\n",
    "            logits_cls = logits[:,0]\n",
    "            loss = train_loss(logits_cls.view(-1, 2), labels.view(-1)) / accum_steps            \n",
    "            total_train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            if (j+1) % accum_steps == 0:\n",
    "                # Clip the norm of the gradients to 1.0.\n",
    "                \n",
    "                losses_track.append(loss*accum_steps)\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                model.zero_grad()\n",
    "                \n",
    "            if j % (3*accum_steps) == 0 :\n",
    "                print('loss',loss*accum_steps)\n",
    "                accuracy, precision, recall, f1_score = test(model, number = 1024)\n",
    "                losses_track.append(loss*accum_steps)\n",
    "                accuracies.append(accuracy)\n",
    "                precisions.append(precision)\n",
    "                recalls.append(recall)\n",
    "                f1_scores.append(f1_score)\n",
    "                global_steps.append(j)\n",
    "        avg_train_loss = torch.tensor([total_train_loss / len(train_loader) * accum_steps]).to(device)\n",
    "        print(f\"Average training loss: {avg_train_loss.mean().item()}\")\n",
    "\n",
    "except RuntimeError as err:\n",
    "    print(err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3e5b372d30>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD6CAYAAACvZ4z8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAyf0lEQVR4nO3deVzVZfr/8dfFrgiCgorgAu7mLmmlZZtlyy9tt6amZvrm1OQ032oWnaVprGn5Nks1OTVWtk1le1nZ2KItLqmYK7iAuACCbAqC7Fy/P87BjnCQgx4EDtfz8TgPz2c7XOjxfe5zf+7P/RFVxRhjjO/ya+0CjDHGtCwLemOM8XEW9MYY4+Ms6I0xxsdZ0BtjjI+zoDfGGB/nUdCLyDQR2SEiaSIyx832fiLypYhsFpGvRCTOZdstIpLqfNzizeKNMcY0TZoaRy8i/sBOYCqQCawDblDVFJd93gY+VtWXReR84CeqerOIdAOSgERAgfXAeFU92NjPi4qK0v79+5/cb2WMMR3M+vXr81U12t22AA+OnwCkqWo6gIgsAqYDKS77DAfudT5fDnzgfH4x8LmqFjqP/RyYBrzR2A/r378/SUlJHpRljDGmjojsbWybJ103sUCGy3Kmc52rTcBVzudXAmEi0t3DYxGRWSKSJCJJeXl5HpRkjDHGU946GfsrYIqIbACmAFlAjacHq+oCVU1U1cToaLffPIwxxpwgT7pusoA+LstxznVHqep+nC16EekCXK2qh0QkCzi33rFfnUS9xhhjmsmTFv06YJCIxItIEDATWOy6g4hEiUjda80FFjqfLwUuEpFIEYkELnKuM8YYc4o0GfSqWg3MxhHQ24C3VDVZROaJyBXO3c4FdojITqAn8BfnsYXAgzg+LNYB8+pOzBpjjDk1mhxeeaolJiaqjboxxpjmEZH1qprobptdGWuMMT7Ogt4c1/sbMjlQXN7aZRhjToIFvWnUtuxi7nlzEw9+nNL0zsaYNsuC3jTqgw2OUbRLtmSzt6D0pF6rsrrWGyUZY06ABb1xq6ZW+XDjfsb1jSDAz48F36Sf8GtlFB5h9J8/Y9YrSdYNZEwrsKA3bq1JLyCnuJyfTo7n6vGxvL0+k9zDJxbS732fRVlVDV/vzOPCv3/NG2v3UVvbtkZ7GePLLOiNW+9vyKJLcAAXDuvJrHMGUFVTy0sr9zT7dVSVDzdmcUZCN/77v+dwWu9w5r63hRuf/449+SfXHWSM8YwFfQeWdaiMqpqGfeflVTX8d2sO00b0IiTQn/ioUC4Z0YtXv9vL4fKqZv2MzZlFpOeXcuXYWOKjQnnj9jN49KqRJO8v5vZXvH+9hKqSlnuYtnZ9iDGtyYK+g9pbUMp5f/2KO/+zvkEofrktl8MV1Vw59oeJRu+YMoDD5dW8vmZfs37O+xuyCArwY9qIGABEhJkT+vLLCwaRmltCTpF3++wXb9rPhX//hp+9up6iI837UDLGV1nQd1D/t3QHVTW1fLEtl//UC+/3N2TRMzyYMxK6H103Ki6CSQO788KK3VRUezYxaVVNLR9t2s+Fw3rQtVPgMdsmxjtee83ugpP8TX6gqiz4Jp2oLkEs257LZf/8lo0Zh7z2+sa0Vxb0PmpTxiHySyrcbtuw7yCfbM5m9nkDmTI4moc+TmHngcMAFJZW8tWOXKaPicXfT4457o4pA8g9XMH732e5e9kGVqTlU1BayYwxDW5BwPDe4YQFB7Bmt/emPvouvZDk/cX86qIhvH3HmajCtc+uYuGK3daVYzo0C3oflHrgMFc9s4rrnl3doPtCVXlkyXaiugTxsykD+Ou1owkLCeDuNzZQXlXDJ1uyqa5Vt+E8eWAUI2LD+ddXuyj2oK/+gw1ZRHQO5NwhPRps8/cTEvtHsibdey36F1ak0y00iBljYxnbN5JP7p7MlME9mPdxCr//YGuzXuuLlANc+uS3rEjN91p9xrQWC3ofo6r8+aMUOgf6k3mwjDtfW3/MCdcvtuWydk8hv7xwMF2CA4gOC+bxa0azPecwj366nQ82ZDG4ZxeGxYQ1eG0R4Q+XDWf/oTJmv76BajcncuuUVFSzNDmHy0bGEBTg/m02Ib47u/JKyTvs/ptHc6TnlfDl9lxuOqMfIYH+AER0DuK5H4/n9rPjeX3NPhZv2u/Ra+3KK+F/39zI9pxibl64hr9/vpOadjAcdHd+KZ9uyW4XtZpTy4Lex3yWcoAVafncd9FgHrlqJKt2FfDHD7aiqlTX1PLop9tIiA5l5uk/3EvmvKE9uPWs/ry0ag/r9x5kxthYRMTt65+R0J0HZ4zgm515/PmjlEa7RD5LzqG8qvaYE7r1TUzoBsBaL3TfvLhyD4F+ftx8Rr9j1osIv502lHF9I/j9+1vIKDxy3Nc5UlnNnf9ZT1CAH5/dM4Wrxsbx1Jep3PT8GnLb+MVec9/bzJ2vfc+0J77hs+Qc664yR1nQ+5Dyqhoe+iSFwT27cNMZ/bh6fByzzxvIonUZPPdtOm8mZbArr5TfThtKoP+x//RzLhnK0F6OVvx0N902rm6Y0JdZ5yTw6nd7eWnVHrf7vL8hi7jITozvF9no64yM7UrnIP+TPiF76Eglb6/PYMbY3kSHBTfYHuDvx5Mzx6IK97y5sdFvIqrKnHe3kJpbwlMzxzKwRxf+dt1oHr9mFBsyDnLpU9/y4cYst0NSm6MlLhbLOlTGd+mFTB3ek5paZdar67nm2dUn9CFaW6t2QVsLq6iu4ZXVeygsrTwlP8+C3oe8sGI3GYVl/On/nUaAM8jvnTqYy0bG8Min23n00+2c3j+Si4b3bHBsSKA/L9x6OgtuHk9sRKcmf9Zvpw1l6vCePPhxCsu2HzhmW25xOSvT8pkxpvFvBgCB/n6M7xd50i3619bso7yqlp9Ojm90nz7dOvPQjBEk7T3I08vT3O7z8qo9LN60n/umDmbyoKij669N7MPi2ZOJ6hLMLxdtZPJjy3h6WSoFjZzsPp4N+w4y4eEveGX1nmYfezyLNzq6pf5w2TCW3nMOD185kozCI1z379Xc9tI6tucUe/xad762npteWOPV+nxZTa1SXuXxLbIBeGXVXu7/MJnbX0lq9rEnwqOgF5FpIrJDRNJEZI6b7X1FZLmIbBCRzSJyqXN9fxEpE5GNzsez3v4FjEN2URlPL0vj4tN6MmngDyHl5yf87brRjIqL4HB5NXMvHdZo+MZGdOKi03p59PP8/YQnZ45hWEw4v3h9A/e8uZF7nY/Zb2ygVmHG2N5Nvs7E+G5szznMQQ9aNluzipj3UQrvb8ikqMxxMriyupZXVu/h7EFRDO0VftzjZ4yN5cqxsTz1ZSpJe374cKmqqWVFaj4PfbKNC4f14OfnDmxw7OCeYSy5+2wW3prIkF7h/PWznZz5yDLmvrfZ4/+oa3cXctPza8gvqeSlVXu82rXywYYsxvWNoF/3UAL9/bhxYl++/vV5/PriIazdU8glT37LvW9tJPPg8buuNmYcYmnyAVbtKmD93oNeq+9UU1WPBgx4w+/e28LIB5Zyy8K1LFq7r8kGQElFNc98vYt+3Tuzfu9Bfvvu5hbvZmvy5uAi4g/MB6YCmcA6EVmsqq5z1/4Bxy0GnxGR4cASoL9z2y5VHePVqk0Dj366nRpV/nDZ8AbbQgL9efW2CaQeKGFc38a7Upqrc1AAL9xyOrNf/56kvce2ymeM6c3AHg1P6NY30TlWf+2eQi5u5ENmT34pf/1sBx9vzsZPoFYh0F84a0AUvSM6caC4gseuHuVRzfOmn0bS3kLu+M96Yrp2Iqe4nPySClShb7fO/O26Mfj5uf8g9PMTzh/ak/OH9iQtt4SXVu3mtTX7yDtcwTM3jW/QHeZqRWo+t7+SRExECFeM7s0TX6SyJauIUXERHtV9PNuyi9lx4DAPTj/tmPWdgvy567yB3DihL898vYuXVu3h403Z/PLCQdx1XsMPM4Cnl6XStVMgqsrCFbuP2/XWlv3ji1T+tTyNn587gLvOH0hwgH+L/JyMwiO8830mI2O7kp5fwpz3tvC797cwMb47f7lyBAnRXRoc8+KK3RSWVrLw1tNZmZbP40t3kBDVhV9eOKhFagQPgh6YAKSpajqAiCwCpgOuQa9AXXOqK+DZ8AbjFev3HuTDjfv5xfkD6dOts9t9wkMCW+Q/ba+uIbxz51knfPyouK4EB/ixJr1h0OeXVPDkF6m8sXYfgf5+zD5vILefk8CuvBKWbs3h0605fL0zj0E9ujBlcLRHPy8sJJD5N47joU+20SnQn+Ex4fTqGkKvriFcMLThhV2NGdijCw/NGMmQnmH88cNkfvPOZv527Wi3HxLLth/gjv98T0JUKK/eNpGgAD/+tXwX72/I8jjol2/P5ft9B7l36uAG38g+2JBFgJ9w2Sj336AiQ4P43aXDuPWs/sz7KIXHl+5gZGxXzqn3d7Y1q4gvtuVy79TBHKmsYcE3u8goPOL2PfXK6j0cKC7n9rMTiOgc5NHvcKrkHi7nuW/SieoSzFPL0vhkSzaPXT2KxP7dvP6zXlixGz+BZ24aR6/wEFKyi1m6NYdXvtvLrFfX8+FdkwgN/iFmi45UseDbdKYO78mYPhGMjutKel4p//hiJ/HRoVwxuulvwSfCk6CPBTJcljOBifX2eQD4TER+AYQCF7psixeRDUAx8AdV/bb+DxCRWcAsgL59+3pcvHH41/I0uocGcee5A1q7lGYLDvBnXN/IBidkyyprmLnAMfHZzAl9uPv8QfQIDwFgXN9IxvWNZM4lQ9lx4DARnYKOey6gvlFxEbz1szO9Uv/NZ/anqKyKv362k/CQAB644rSjteQdruC1NXuZvzyNob3CeeWnE4gMdYTiBcN68NGm/fz+0mFHz6c0ZntOMXe+tp7yqlpiIzoxc8IP/0fqppOeMjiabqHHD9zeEZ14YuYYLnvqW+a8u5ml95xDWMgPH2z/XJZKWEgAt5zVnyOV1Tz/bTovrdrDHy8/9lvi2t2F/GlxMqrwyuq93DFlAD+dFE+noJZpNTfXM1/torKmljdmncHeglJ+//5Wrv33am4+ox+/ungI4SFNf5jX1Cr5JRXkFJUTHRZMbzfnrQpLK1m0bh/Tx8QS09Wx/bTeXTmtd1fOSOjOTS+s4TfvbubpG8YefU/8+5tdlFRUc+/UwYBjVNjDV40go/AIv3p7E7ERxx/AcKK8dTL2BuAlVY0DLgVeFRE/IBvoq6pjgXuB10WkQUeqqi5Q1URVTYyO9qxlZhx2uYwf7xzkyed22zMhvhsp2cVH+90BHvokhbTcEl78yek8NGPk0ZB3JSIM7eVokbemu84byO1nx/Py6r384/OdbM48xL1vbmTSo8t44otUzh3Sg//8z8SjIQ+O8wX5JZWsSDv+BVnF5VXc8er6o9/IHvpkG1mHyo5ur5tOesZxhrG6Cgn05/FrR5NTXM7DS7YdXb89p5ilyQf4yaR4unYKJKZrJy4bFcOb6zKOmciurLKG37yzibjITrz387OY0L8bjy/dwZTHl/PG2n2tPqRz/6EyXvtuH9eMiyM+KpRzh/Tgs3vO4ZYz+/Pqd3uZ8n/L3U7jUVldy4cbs/jR899x5iNfMvgPnzLx4S+ZPn8l5/31K1L2NzyZ/dKqPZRX1XLHlIQG284aGMWvLx7KJ5uzWeic9TW/pIIXV+7h8lG9GRbzQwwGB/jz7M3jiekawm/f3dwi10F4EvRZQB+X5TjnOle3AW8BqOpqIASIUtUKVS1wrl8P7AIGn2zRrU1VWbUrv9kzObaEF1fuJijAj5vqjR9vTyYmdEMV1jv7+Zcm5/Damn387JwEzh7U9j/4RYTfXTqM6xP78NSyNK54eiVLk3O4cWJfvrxvCs/9OLFBl9C5Q6IJDwk4ehcvd1SVX721icyDZcz/0TieuH4MtarMfW/L0UB1nU7aU+P6RnL72Qm8sTaDb3bmAfDPZWl0CQ7gp5P6H93vtsnxlFRU8+a6H77QP750B3sKjvDY1aMY1zeSF249nbfvOJM+3Toz970tvOvh9Bgt5Z/LUlGUX1zwwzmI0GDHN62PZk9mRGxXHvw4hfP/+jXvrs8kt7icJ79IZdJjy/jloo3sP1TOpIFR3DllAA/NGMGzN40jonMgd762/piGSGlFNa+s3sPU4T0bPRd1x5QELhrek0eWbGPdnkL+tXwXFdU13OOmL75baBAv3no6z/84scHUI97gSRNwHTBIROJxBPxM4MZ6++wDLgBeEpFhOII+T0SigUJVrRGRBGAQcOK3Kmoj1uwu5Mbn1hAa5M814+P48Vn9GeDmpEtLO1hayTvrM5kxxv348fZiXN9Igvwd/fTDY7ry23c3MyI2nPsuGtLapXnM8RV8JD3Dg4kMDeKa8XHHdIvUFxzgz2WjevPBhixKK6qP6cet8+zX6XyWcoA/Xj6c0539y3MuGcr9HybzVlIG08fE8qlzOunmdpvcM3UwX2w7wJx3NzP/R+NYsiWbO6cMOKa/fVRcBBPiu/Hiyj3celZ/NmQc4sVVu7n5jH6cNeCHkV2n9+/G2z87k6ueWcVj/93OtBG96OLm92lKeVUNW7OKyDpUxtBe4Qzs0aVZobcnv5S3kjK5aWJf4iIbnlcYEduVV2+byIrUfB7773bue3vT0W3nDonm1rP6c86g6AbnWaLDgrn+399x31sbWXBzIn5+wqJ1GRw6UnXc7lIR4a/XjWb60yv5+WvfU1RWxdXj4tyeoAUaXe8NTf5rqGq1iMwGlgL+wEJVTRaReUCSqi4G7gOeE5F7cJyYvVVVVUTOAeaJSBVQC9yhqt6bxaqVbHLOiHj+sJ68sTaDl1fvZcrgaKaN6EWM88ReTHgnwjsFcOhIFdlF5RwoLienuJxxfSMZ0qvp0SieeH2tY/z4bZMbfnVsT0IC/Rndpyur0wvYklVERVUtT84c2+jUCW2Vv59wbzM+nK4cG8sba/fxecqBBl0vq9LyeXzpdi4fFXNMK/umif1YsiWbhz7eRnFZNSX1ppP2VF0XzjXPrOJHz6+hU6A//3N2w/fRbZPj+dmr6/lg436eXpZKXGQn5lwytMF+fn7Cn684jenzV/LPZanMvWSYR3Ws3V3I4k1ZbMw4xPbsw1S7dFuEBvkzKi6CMX0juHFC30YHGtR58stUAv2l0RFFdSYPiuKsAZP4dGsOO3KKmTE29rghO75fN35/2TD+/FEKz3y9i1nnJPDCt+lMiO/W5Ci28JBAnrlpHDPmr0RVufuClhtZczwefeyq6hIcQyZd193v8jwFmOTmuHeBd0+yxjYnJbuYmK4h/POGseQdruCNtfv4z3d7+dr5NbhO3VBAV4N6dOGze85p1slDdyqra3l5lWP8uLc+OFrTxPjuRy9keuzqka3yDelUS+wXSWxEJ97fkHVM0K/alc9dr39PQnQXHrt61DHvFT8/4fFrRnPxE9/wlyXbGkwn3Rx1XTj//iadn52T4PZk7oXDetKve2fmvLuZ6lrl9dsnuv32ATC6TwTXjo9j4YrdXJ/Yp8kWanZRGTe/sIZAfz9GxXVl1jkJjOkTQZ9undmWXczGjENszDjEc9+ks2xbLh/fPbnRIaw7Dxzmg41ZzDo7we35nPr8/ITLRsVw2aiYJvcFHN9o9h3ib5/tYF/BEfYXlfOXK0d6dOzQXuG8/JMJHDxS2eSHVUtpn2fvWlny/mJO6+04mRIdFszdFwzi5+cOIKfY0XLPLionp6icg0cq6RYaTK9wRys/aU8hj3y6nQ0Zh056PPvHm/eTe7iCx68d7Y1fqdVNTOjG08vhkhG9uC6xT9MH+AA/P2HG2N4889Uu8g5X0C00iKeXpfHklzuJjwrluR8nug3VPt06M/eSofzxw2S300k3xz1TB9O3e+dGh/X5+wk/nRTPnxYnN+iycefX04bw6dYcHvpkGwtvPf24+/79s52owqe/PLtBAA6LCeeqcXEAfJ5ygNtfSeL5b3c32lXyt892EBoUwB1TWmbkmYjwyFUj2ZZdzJtJGQztFca5Qzw/fzTxBD+MvcWCvpnKKmtIzyvh0pHHtgQC/P2Ii+zstm+wzuCeXXjii1TeWpfhNuhra5Wrn13FgaJyLh7Ri2mn9SKxf7cG/5FVlee/3c2gHl04Z9Dx/+O1F2cNiOLhK0dy+eiYk/62057MGBPL/OW7eHHlbjZlHmJlWgFXjo3loRkjGm05A/xoYj+CA/y5YFjDKaCbIyTQnx9NPP6J/Bsm9KVrp8BGL2hz1SMshLsvGMjDS7azfEcu57mZohoco3ze+T6T2ybFN9nKnTq8JxcN78mTX+7k8lExDfZ/ZfUeliYf4FcXDT5mZJO3hQYH8OzN45n1ShK/mTakXb1P21cnaBuwPaeYWoXhMce/3N6dsJBALh8Vw0eb9lNaUd1g+2cpOWzYd4ge4SG8tmYf1y/4jgl/+YLfvrOZt5IySD1wmNpa5bv0QlKyi7ltcny7erMdj7+fcOPEvh6NcfYlg3qGcVpvxxz/SXsO8tjVI/n7daOPG/Lg+DZw3el96N6l5U/CBwX4MWNsrMcnfG89K56EqFAe/CiFymr3E8A9smQ7YcEBzD7/+P3pdR644jT8Rbj/w63HDOH8akcuDyxO5sJhPbjTzdQV3jYgugtf3ncu5w/1fJRTW2At+mZKdo6nreu6aa7rT+/D2+sz+WRzNte5TBWsqjz1ZRoJUaG8e+dZlFXV8NWOXP67NYclW7J5M8kxxC0sOICQIH+6O2+wYdq/n587kJdX7WHejNOanK+nPQgK8OOP/284P3lxHf9cltrgat4Vqfl8vTOP31061OOrantHdOLei4bw4McpfLo1h0tHxrAj5zCzX9/AkF7hPDlzbIsMS/QVFvTNlLy/mK6dAomLbHqGR3fG94tkQHQoi9btOybov9yWS0p2MX+7djT+fkKX4AAuH9Wby0f1prZWSc8vYcO+Q2zKPMSWrGKuS4w7eoMN074156Rge3HekB5MH9Obfy5LY3d+KY9cNZKwkEBqa5VHPt1GbEQnfnxm/2a95i1n9uO97zN5YHEyw2LC+elL6+gc5M8Lt7g/l2F+YH87zZSSXczwmPAT7jIREa4/vQ8PL9lO6oHDDOoZ5mjNL0ulb7fOTB/T8KSYn58wsEcYA3uEcW0HOVFp2r9/XDeGIb3C+NtnO9maVcTTN44jNfcwyfuLeeL6Mc1uqAT4+/HwlSOZ8a+VXPrktyjKWz870+30BOZY1kffDNU1tWzPLmb4CXbb1LlqXBwBfnL0isOvduaxObOIu84b0OS8J8a0F35+ws/PHcgbt59BeVUtVz2zigc/3saI2PATnrxrdJ8IbjmzP2VVNfzjujFemf2zI7BUaYb0/FIqqmtPuH++TlSXYC4c1pP3NmRRWV3LU1+mEhvRiSvHxnmpUmPajgnx3fjk7smcmdCdg0cq+d2lwxqdCtoTf7x8ON/+5jwuGelb3V0tybpumiF5fxHgmKHuZF0/oQ//Tc7hgY+S2bDvEA/NGNHurgQ1xlPduwTz4q2nc+Bw+dGZHk+Uv5+02oVH7ZUlSzOk7C8mKMCPhOjQk36tcwZFE9M1hNfX7KNXeAjXJlpr3vg2Pz856ZA3J8aCvhmS9xcztFfYce8k5Cl/P+Ga8Y5wv2NKQovdAccYY6zrxkOqSvL+Yi4d6dk9VT1x2+R4QgL9uWGi3WzFGNNyLOg9tL+onKKyKoZ7oX++TkTnoCZn2jPGmJNlXTceSs5ynIg9kakPjDGmNVnQeyh5fzEiMCym/U8JbIzpWDwKehGZJiI7RCRNROa42d5XRJaLyAYR2Swil7psm+s8boeIXOzN4k+llOxiEqJC2+19WY0xHVeTQS8i/sB84BJgOHCDiAyvt9sfgLecNwGfCfzLeexw5/JpwDTgX87Xa3dS9hd7tX/eGGNOFU9a9BOANFVNV9VKYBEwvd4+CtR1XncF9jufTwcWOW8SvhtIc75eu3KwtJKsQ2UnfUWsMca0Bk+CPhbIcFnOdK5z9QBwk4hk4rjl4C+acSwiMktEkkQkKS8vr/7mVpeSfXJTExtjTGvy1snYG4CXVDUOuBR4VUQ8fm1VXaCqiaqaGB3t+e25TpUU5xz0NuLGGNMeeXJmMQtwnRs3zrnO1W04+uBR1dUiEgJEeXhsm5eSXUyv8JBTcjcfY4zxNk9a3euAQSISLyJBOE6uLq63zz7gAgARGQaEAHnO/WaKSLCIxAODgLXeKv5UyTx4hH7dbRIlY0z71GSLXlWrRWQ2sBTwBxaqarKIzAOSVHUxcB/wnIjcg+PE7K3quLFjsoi8BaQA1cBdqlrTUr9MSykoqWSY9c8bY9opjwaFq+oSHCdZXdfd7/I8BZjUyLF/Af5yEjW2urySCs5uwbvLG2NMS7IrY5tQUV3D4fJqoqx/3hjTTlnQN6GgpBLATsQaY9otC/om1AV9VBfrujHGtE8W9E3IL6kArEVvjGm/LOibUBf00Rb0xph2yoK+Cfl1XTdh1nVjjGmfLOibUFBSQadAf5ue2BjTblnQNyG/pMJa88aYds2Cvgn5JZV0D7X+eWNM+2VB34T8kgq7WMoY065Z0Dchv6TSxtAbY9o1C/rjqK1VCkutRW+Mad8s6I/j4JFKahW6W4veGNOOWdAfR0Fp3fQH1qI3xrRfFvTHkX+4bvoDa9EbY9ovC/rjyHe26G36A2NMe+ZR0IvINBHZISJpIjLHzfZ/iMhG52OniBxy2Vbjsq3+LQjbtB9a9Bb0xpj2q8nr+kXEH5gPTAUygXUisth5VykAVPUel/1/AYx1eYkyVR3jtYpPoYLSCvz9hIhOga1dijHGnDBPWvQTgDRVTVfVSmARMP04+98AvOGN4lpb/uFKuoUG4ecnrV2KMcacME+CPhbIcFnOdK5rQET6AfHAMpfVISKSJCLficiMRo6b5dwnKS8vz7PKT4ECG0NvjPEB3j4ZOxN4R1VrXNb1U9VE4EbgCREZUP8gVV2gqomqmhgdHe3lkk5cnl0Va4zxAZ4EfRbQx2U5zrnOnZnU67ZR1Sznn+nAVxzbf9+m5R+2Fr0xpv3zJOjXAYNEJF5EgnCEeYPRMyIyFIgEVrusixSRYOfzKGASkFL/2LZIVSkoraB7qLXojTHtW5OjblS1WkRmA0sBf2ChqiaLyDwgSVXrQn8msEhV1eXwYcC/RaQWx4fKo66jddqy0soayqtqiQqzFr0xpn3z6LZJqroEWFJv3f31lh9wc9wqYORJ1NdqCpz3irWuG2NMe2dXxjai7qbgNv2BMaa9s6BvRN1NwW36A2NMe2dB3whr0RtjfIUFfSMKnC16u1+sMaa9s6BvRH5JBeEhAQQF2F+RMaZ9sxRrREFJpQ2tNMb4BAv6RuSVVBBl3TbGGB9gQd+IgpIKosLsRKwxpv2zoG9EfkmlnYg1xvgEC3o3KqtrKSqrsqtijTE+wYLejULnvWJtDL0xxhdY0LuRb/PcGGN8iAW9Gz8EvbXojTHtnwW9G3Xz3FiL3hjjCyzo3SiweW6MMT7Eo6AXkWkiskNE0kRkjpvt/xCRjc7HThE55LLtFhFJdT5u8WLtLSa/pILgAD+6BHs0Xb8xxrRpTSaZiPgD84GpQCawTkQWu94pSlXvcdn/FzjvCysi3YA/AYmAAuudxx706m/hZQUllUR1CUZEWrsUY4w5aZ606CcAaaqarqqVwCJg+nH2v4EfbhB+MfC5qhY6w/1zYNrJFHwq5JVU2IlYY4zP8CToY4EMl+VM57oGRKQfEA8sa+6xbUldi94YY3yBt0/GzgTeUdWa5hwkIrNEJElEkvLy8rxcUvPll1TYiVhjjM/wJOizgD4uy3HOde7M5IduG4+PVdUFqpqoqonR0dEelNRyamuVwlJr0RtjfIcnQb8OGCQi8SIShCPMF9ffSUSGApHAapfVS4GLRCRSRCKBi5zr2qyisiqqa5XuFvTGGB/R5KgbVa0Wkdk4AtofWKiqySIyD0hS1brQnwksUlV1ObZQRB7E8WEBME9VC737K3iXXRVrjPE1Hg0UV9UlwJJ66+6vt/xAI8cuBBaeYH2n3IFiR9D3DA9p5UqMMcY77MrYerKLygCI6WpBb4zxDRb09eQUlQPWojfG+A4L+npyisuJ7BxISKB/a5dijDFeYUFfT05ROb26dmrtMowxxmss6OvJLiq3/nljjE+xoK8np7icXhb0xhgfYkHvoryqhsLSSnrZiVhjjA+xoHeR6xxDby16Y4wvsaB3YWPojTG+yILeRU6xYwy9Bb0xxpdY0LvItouljDE+yILeRU5ROV2CAwgLCWztUowxxmss6F04Lpay1rwxxrdY0LvILraLpYwxvseC3kVOUZmNoTfG+BwLeqfqmlryDldY140xxud4FPQiMk1EdohImojMaWSf60QkRUSSReR1l/U1IrLR+WhwC8K2Iq+kglq1i6WMMb6nyTtMiYg/MB+YCmQC60RksaqmuOwzCJgLTFLVgyLSw+UlylR1jHfL9r66oZXWR2+M8TWetOgnAGmqmq6qlcAiYHq9fW4H5qvqQQBVzfVumS2v7oYjvcJtimJjjG/xJOhjgQyX5UznOleDgcEislJEvhORaS7bQkQkybl+hrsfICKznPsk5eXlNad+rzka9NaiN8b4GI9uDu7h6wwCzgXigG9EZKSqHgL6qWqWiCQAy0Rki6rucj1YVRcACwASExPVSzU1S05xOUEBfkR2touljDG+xZMWfRbQx2U5zrnOVSawWFWrVHU3sBNH8KOqWc4/04GvgLEnWXOLqLvhiIi0dinGGONVngT9OmCQiMSLSBAwE6g/euYDHK15RCQKR1dOuohEikiwy/pJQAptkI2hN8b4qiaDXlWrgdnAUmAb8JaqJovIPBG5wrnbUqBARFKA5cCvVbUAGAYkicgm5/pHXUfrtCV2C0FjjK/yqI9eVZcAS+qtu9/luQL3Oh+u+6wCRp58mS2rtlbJLa6gpwW9McYH2ZWxQOGRSipraomxrhtjjA+yoMd1aKWNoTfG+B4LeuyqWGOMb7Ogx24haIzxbRb0OIZW+vsJ3bsEt3YpxhjjdRb0OLpueoYF4+9nF0sZY3yPBT12C0FjjG+zoMfRRx9jI26MMT6qwwe9qpJTVE5PG0NvjPFRHT7oi8urOVJZYyNujDE+q8MHvc1Db4zxdRb0NobeGOPjLOiLygBr0RtjfFeHD/q66Q96hFnQG2N8U4cP+pyicqK6BBMU0OH/KowxPqrDp9u+wiPERtoYemOM7/Io6EVkmojsEJE0EZnTyD7XiUiKiCSLyOsu628RkVTn4xZvFe4tu/NLGRAV2tplGGNMi2nyDlMi4g/MB6biuAn4OhFZ7HpLQBEZBMwFJqnqQRHp4VzfDfgTkAgosN557EHv/yrNd6SymuyichKiLeiNMb7Lkxb9BCBNVdNVtRJYBEyvt8/twPy6AFfVXOf6i4HPVbXQue1zYJp3Sj95u/NLAYiP6tLKlRhjTMvxJOhjgQyX5UznOleDgcEislJEvhORac04FhGZJSJJIpKUl5fnefUnKT3PEfTWojfG+DJvnYwNAAYB5wI3AM+JSISnB6vqAlVNVNXE6OhoL5XUtLoWff/uFvTGGN/lSdBnAX1cluOc61xlAotVtUpVdwM7cQS/J8e2mvS8EmIjOtEpyL+1SzHGmBbjSdCvAwaJSLyIBAEzgcX19vkAR2seEYnC0ZWTDiwFLhKRSBGJBC5yrmsTdueXEm8jbowxPq7JoFfVamA2joDeBrylqskiMk9ErnDuthQoEJEUYDnwa1UtUNVC4EEcHxbrgHnOda1OVUnPK7X+eWOMz2tyeCWAqi4BltRbd7/LcwXudT7qH7sQWHhyZXpffkklhyuqrUVvjPF5HfbK2PS8EgASom1opTHGt3XYoK8bcZNgLXpjjI/rsEGfnl9KUIAfvSNsnhtjjG/ruEGfV0r/7p3x95PWLsUYY1pUxw36/BISbOoDY0wH0CGDvrqmln0FR4i3oZXGmA6gQwZ9xsEyqmvVTsQaYzqEDhn0u/PrhlZa0BtjfF+HDPqjs1ZaH70xpgPomEGfX0pE50AiQ4NauxRjjGlxHTPo80qsf94Y02F0yKB3zFpp3TbGmI6hwwV9SUU1B4or7ESsMabD6HBBvzvP5rgxxnQsHS7o0/Nt1kpjTMfiUdCLyDQR2SEiaSIyx832W0UkT0Q2Oh//47KtxmV9/TtTnXLpeaWIQL/unVu7FGOMOSWavPGIiPgD84GpOO4Nu05EFqtqSr1d31TV2W5eokxVx5x0pV6yO7+U2IhOhATafWKNMR2DJy36CUCaqqaraiWwCJjesmW1nPT8ErurlDGmQ/Ek6GOBDJflTOe6+q4Wkc0i8o6I9HFZHyIiSSLynYjMcPcDRGSWc5+kvLw8j4tvLlVld14pA6x/3hjTgXjrZOxHQH9VHQV8Drzssq2fqiYCNwJPiMiA+ger6gJVTVTVxOjoaC+V1FBOcTmllTXWojfGdCieBH0W4NpCj3OuO0pVC1S1wrn4PDDeZVuW88904Ctg7EnUe1JWpOYDkNg/srVKMMaYU86ToF8HDBKReBEJAmYCx4yeEZEYl8UrgG3O9ZEiEux8HgVMAuqfxD1llu/IpWd4MMNjwlurBGOMOeWaHHWjqtUiMhtYCvgDC1U1WUTmAUmquhi4W0SuAKqBQuBW5+HDgH+LSC2OD5VH3YzWOSWqamr5dmc+l42KQcRuH2iM6TiaDHoAVV0CLKm37n6X53OBuW6OWwWMPMkavSJpz0EOV1Rz3tAerV2KMcacUh3mytjlO3IJ9BcmDYxq7VKMMeaU6jBBv2x7LhPju9Ml2KMvMcYY4zM6RNBnFB4hLbfEum2MMR1Shwj65TtyAThvSMuN0TfGmLaqYwT99lz6d+9sM1YaYzoknw/6ssoaVu0qsG4bY0yH5fNBvzo9n4rqWs4bYkFvjOmYfD7ol2/Po1OgPxMTurV2KcYY0yp8OuhVlWXbc5k0MIrgAJt/3hjTMfl00KfmlpB1qIzzrX/eGNOB+XTQL9/uHFY51IZVGmM6Lp8O+m9S8xjSM4yYrp1auxRjjGk1Phv05VU1rNtzkLMH2dw2xpiOzWeDft2eQiqra5lkQW+M6eB8NuhXpOUT6C9M6G/DKo0xHZvPBv3KtHzG9o0k1GarNMZ0cB4FvYhME5EdIpImInPcbL9VRPJEZKPz8T8u224RkVTn4xZvFt+YwtJKkvcXc7bNPW+MMU3fYUpE/IH5wFQgE1gnIovd3BLwTVWdXe/YbsCfgERAgfXOYw96pfpGrN5VgCrWP2+MMXjWop8ApKlquqpWAouA6R6+/sXA56pa6Az3z4FpJ1aq51ak5REWHMCo2K4t/aOMMabN8yToY4EMl+VM57r6rhaRzSLyjoj0ac6xIjJLRJJEJCkvL8/D0hu3Ii2fMwZ0J8DfZ09BGGOMx7yVhB8B/VV1FI5W+8vNOVhVF6hqoqomRkef3FWs+wqOkFFYZuPnjTHGyZOgzwL6uCzHOdcdpaoFqlrhXHweGO/psd62Ii0fwG4CbowxTp4E/TpgkIjEi0gQMBNY7LqDiMS4LF4BbHM+XwpcJCKRIhIJXORc12JWpOUR0zWEhKjQlvwxxhjTbjQ56kZVq0VkNo6A9gcWqmqyiMwDklR1MXC3iFwBVAOFwK3OYwtF5EEcHxYA81S1sAV+DwBqapVVuwq4cFhPRKSlfowxxrQrHl1NpKpLgCX11t3v8nwuMLeRYxcCC0+iRo+l7C/m0JEqJlu3jTHGHOVTw1Ksf94YYxrysaDPY2ivMKLDglu7FGOMaTN8JujrpiW21rwxxhzLZ4K+uKyKaaf14gK7baAxxhzDZ6Z27BEewlM3jG3tMowxps3xmRa9McYY9yzojTHGx1nQG2OMj7OgN8YYH2dBb4wxPs6C3hhjfJwFvTHG+DgLemOM8XGiqq1dwzFEJA/YexIvEQXke6mcU6G91QtW86nS3mpub/WCb9XcT1Xd3qKvzQX9yRKRJFVNbO06PNXe6gWr+VRpbzW3t3qh49RsXTfGGOPjLOiNMcbH+WLQL2jtApqpvdULVvOp0t5qbm/1Qgep2ef66I0xxhzLF1v0xhhjXFjQG2OMj/OZoBeRaSKyQ0TSRGROa9fjjogsFJFcEdnqsq6biHwuIqnOPyNbs8b6RKSPiCwXkRQRSRaRXzrXt8m6RSRERNaKyCZnvX92ro8XkTXO98ebIhLU2rXWJyL+IrJBRD52LrfpmkVkj4hsEZGNIpLkXNcm3xd1RCRCRN4Rke0isk1EzmyrNYvIEOffbd2jWET+90Tq9YmgFxF/YD5wCTAcuEFEhrduVW69BEyrt24O8KWqDgK+dC63JdXAfao6HDgDuMv5d9tW664AzlfV0cAYYJqInAE8BvxDVQcCB4HbWq/ERv0S2Oay3B5qPk9Vx7iM626r74s6TwL/VdWhwGgcf99tsmZV3eH8ux0DjAeOAO9zIvWqart/AGcCS12W5wJzW7uuRmrtD2x1Wd4BxDifxwA7WrvGJur/EJjaHuoGOgPfAxNxXEkY4O790hYeQJzzP+35wMeAtIOa9wBR9da12fcF0BXYjXMQSnuo2aXGi4CVJ1qvT7TogVggw2U507muPeipqtnO5zlAz9Ys5nhEpD8wFlhDG67b2QWyEcgFPgd2AYdUtdq5S1t8fzwB/AaodS53p+3XrMBnIrJeRGY517XZ9wUQD+QBLzq7yJ4XkVDads11ZgJvOJ83u15fCXqfoI6P6DY53lVEugDvAv+rqsWu29pa3apao46vu3HABGBo61Z0fCJyOZCrqutbu5Zmmqyq43B0md4lIue4bmxr7wsgABgHPKOqY4FS6nV7tMGacZ6buQJ4u/42T+v1laDPAvq4LMc517UHB0QkBsD5Z24r19OAiATiCPnXVPU95+o2X7eqHgKW4+j2iBCRAOemtvb+mARcISJ7gEU4um+epG3XjKpmOf/MxdF3PIG2/b7IBDJVdY1z+R0cwd+WawbHB+n3qnrAudzsen0l6NcBg5yjFIJwfM1Z3Mo1eWoxcIvz+S04+sDbDBER4AVgm6r+3WVTm6xbRKJFJML5vBOO8wnbcAT+Nc7d2ky9AKo6V1XjVLU/jvfuMlX9EW24ZhEJFZGwuuc4+pC30kbfFwCqmgNkiMgQ56oLgBTacM1ON/BDtw2cSL2tfZLBiycrLgV24uiP/X1r19NIjW8A2UAVjtbFbTj6Yr8EUoEvgG6tXWe9mifj+Gq4GdjofFzaVusGRgEbnPVuBe53rk8A1gJpOL4CB7d2rY3Ufy7wcVuv2VnbJucjue7/XFt9X7jUPQZIcr4/PgAi23LNQChQAHR1Wdfsem0KBGOM8XG+0nVjjDGmERb0xhjj4yzojTHGx1nQG2OMj7OgN8YYH2dBb4wxPs6C3hhjfNz/B/voPSbyTSaxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GLOBAL TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision 0.7893627954779034\n",
      "Recall 0.5120333333333333\n",
      "F1_score 0.6211484027496967\n",
      "Accuracy  0.84385\n"
     ]
    }
   ],
   "source": [
    "final_results = {}\n",
    "res = test(model = model, all = True)\n",
    "final_results['accuracy'] =  res[0]\n",
    "final_results['precision'] = res[1]\n",
    "final_results['recall'] = res[2]\n",
    "final_results['f1_score'] = res[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAVE EVERYTHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.dump(accuracies,open(path+'accuracy.p','wb'))\n",
    "p.dump(losses_track,open(path+'loss.p','wb'))\n",
    "p.dump(precisions,open(path+'precision.p','wb'))\n",
    "p.dump(recalls,open(path+'recall.p','wb'))\n",
    "p.dump(f1_scores,open(path+'f1_score.p','wb'))\n",
    "p.dump(parameters, open(path+'parameters.p', 'wb'))\n",
    "p.dump(final_results, open(path+'final_results.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAVE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), path+'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
